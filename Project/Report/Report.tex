\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Autonomous closed-loop guidance using reinforcement learning in a low-thrust, multi-body dynamical environment*\\
{\footnotesize \textsuperscript{*}Reinforcement Learning Course Project Report}
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Ali Baniasad}
\IEEEauthorblockA{\textit{Department of Aerospace Engineering} \\
\textit{Sharif University of Technology}\\
Tehran, Iran \\
ali\_baniasad@ae.sharif.edu}
\and
\IEEEauthorblockN{Mehrdad Boroushaki}
\IEEEauthorblockA{\textit{Department of Energy Engineering} \\
\textit{Sharif University of Technology}\\
Tehran, Iran \\
boroushaki@sharif.edu}
}

\maketitle

\begin{abstract}
    Onboard autonomy is an essential component in enabling increasingly complex missions into deep space. In nonlinear dynamical environments, computationally efficient guidance strategies are challenging. Many traditional approaches rely on either simplifying assumptions in the dynamical model or on abundant computational resources. This research effort employs reinforcement learning, a subset of machine learning, to produce a 'lightweight' closed-loop controller that is potentially suitable for onboard low-thrust guidance in challenging dynamical regions of space. The results demonstrate the controller's ability to directly guide a spacecraft despite large initial deviations and to augment a traditional targeting guidance approach. The proposed controller functions without direct knowledge of the dynamical model; direct interaction with the nonlinear equations of motion creates a flexible learning scheme that is not limited to a single force model, mission scenario, or spacecraft. The learning process leverages high-performance computing to train a closed- loop neural network controller. This controller may be employed onboard to autonomously generate low-thrust control profiles in real-time without imposing a heavy workload on a flight computer. Control feasibility is demonstrated through sample transfers between Lyapunov orbits in the Earth-Moon system. The sample low- thrust controller exhibits remarkable robustness to perturbations and generalizes effectively to nearby motion. Finally, the flexibility of the learning framework is demonstrated across a range of mission scenarios and low-thrust engine types.
\end{abstract}

\begin{IEEEkeywords}
    Reinforcement learning-based, computationally efficient closed-loop control enables autonomous low-thrust guidance in complex deep-space missions, showcasing flexibility across diverse scenarios without relying on explicit dynamical models.
\end{IEEEkeywords}

\section{Introduction}
Advancements in onboard autonomy are enabling new opportuni- ties for establishing a sustained human and robotic presence in deep space. In complex multi-body dynamical environments, such as in the Earthâ€“Moon neighborhood, onboard applications for low-thrust spacecraft are particularly challenging. This investigation demonstrates Reinforcement Learning (RL), a subset of Machine Learning (ML), to be an effective approach for automated closed-loop guidance in these challenging regions of space. Onboard autonomous guidance requires a computationally efficient approach that addresses nonlinearity in the dynamical model and offers flexibility as requirements change inflight. In satisfying these criteria, RL provides a model-agnostic approach for training a neural network controller that is applicable to multiple problems, and potentially suitable for onboard use.
\section{Problem formulation}
The three-body problem serves as a suitable dynamical model that is representative of observed natural motion in cislunar space. While the proposed guidance framework does not depend on any particular model, the planar Circular Restricted Three-Body Problem (CR3BP) serves as a useful environment for preliminary evaluation because it both represents a challenging region of space that is relevant to upcoming missions while sufficiently low-fidelity for initial analysis of the guidance scheme. Additionally, low-thrust propulsion is included to demonstrate algorithmic performance despite limited control authority and pronounced nonlinearities. While the proposed guidance scheme directly supplies a control history, an alternative concept of operations leverages the trained neural network to produce an initial guess for other numerical methods, such as targeting or optimization schemes. A sample direct multiple shooting algorithm is included in this analysis to demonstrate the added value of the neural network to the onboard targeting capability.

\subsection{Dynamical model}

The CR3BP is a model for the motion of an infinitesimal mass moving under the influence of two celestial bodies. In this model, two spherically symmetric gravitational bodies, \(P_1\) and \(P_2\) form the primary system as they move in circular orbits about their common barycenter, B; \(P_3\) moves freely with respect to the barycenter. The motion of \(P_3\) is governed by the following equations of motion:
\begin{equation}
    \ddot{x} - 2\dot{y} = \frac{\partial U}{\partial x} + u\frac{\partial U}{\partial \dot{x}}
\end{equation}
\begin{equation}
    \ddot{y} + 2\dot{x} = \frac{\partial U}{\partial y} + u\frac{\partial U}{\partial \dot{y}}
\end{equation}
where \(U\) is the effective potential function and \(u\) is the dimensionless thrust parameter. The effective potential function is defined as:
\begin{equation}
    U = \frac{1}{2}(x^2 + y^2) + \frac{1 - \mu}{r_1} + \frac{\mu}{r_2}
\end{equation}

\section{Guidance framework design}
\subsection{Reinforcement learning overview}
Reinforcement Learning (RL) is a branch of machine learning that encompasses a broad range of goal-oriented algorithms that 'learn' to perform tasks by means of trial-and-error. Current state-of-the-art RL approaches employ modern advancements in neural networks to aid in challenging tasks. Policy gradient methods are of particular recent interest due to their demonstrated ability in continuous control tasks. One such algorithm, Proximal Policy Optimization (PPO), is employed in this investigation to train a neural network controller.
\subsubsection{Neural Network}
A Neural Network (NN) is a class of nonlinear statistical models that are frequently employed in ML classification and regression tasks [44]. The term neural network encompasses many different types of statistical models with various levels of complexity. When applied correctly, NNs perform exceedingly well, and are a driving factor in ML advancements over the past decade. While frequently used in supervised learning applications, NNs are also employed extensively in modern RL algorithms due to their demonstrated ability in approximating nonlinear functions. Many traditional tabular RL approaches, such as Q-learning, rely on finely discretizing the state and action spaces, quickly becoming impractical as the number of dimensions in the problem increases. Leveraging NNs allows modern algorithms to both access continuous state and action spaces and to easily incorporate additional dimensions.
Evaluating a feedforward neural network consists of straightforward linear algebra operations, with several nonlinear element-wise activation functions. After the input layer, each node is computed as a linear combination of weighted values and a bias, and processed through an activation function to incorporate nonlinearity into the model [44]. The weights signify the impact that particular nodes exert on each node in the next layer. The bias allows the activation function to be shifted left or right for each node. Together, the weights and biases form the set of trainable parameters for the model. In general, these parameters cannot be known a-priori, and so no suitable initial guess for their value is possible. Hence, the weights in this investigation are randomly initialized according to a normal distribution of zero mean and the biases are initialized to zero.
Without activation functions, the network is only able to model linear functions and, thus, the selection of the activation function is an important component in neural network performance. Furthermore, bounded functions are advantageous since they aid in normalizing the output of each neuron. This investigation employs hyperbolic tangent (tanh) to bound outputs and incorporate nonlinearity. Linear activation is also, at times, advantageous. Within RL, networks that produce a single scalar output value frequently employ a linear activation in the output layer. Together, tanh and linear are the only two activation functions employed in this investigation, with \(\tanh\) used in all hidden layers and linear used in the output layer.
\end{document}
